{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network on WIKI-TEXT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import prepare_data\n",
    "import math\n",
    "import time\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder='./data/wikitext-2'\n",
    "\n",
    "corpus = prepare_data.Corpus(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    data = data.cuda()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size= 33278\n",
      "torch.Size([104431, 20])\n",
      "torch.Size([12278, 20])\n"
     ]
    }
   ],
   "source": [
    "batch_size=20\n",
    "bs=batch_size\n",
    "\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, batch_size)\n",
    "test_data = batchify(corpus.test, batch_size)\n",
    "\n",
    "vocab_size = len(corpus.dictionary)\n",
    "\n",
    "print('vocab size=',vocab_size)\n",
    "print(train_data.size())\n",
    "print(test_data.size())\n",
    "\n",
    "train_length = train_data.size(0)\n",
    "test_length  = test_data.size(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With or without GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "#device= torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a recurrent net class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class recurrent_net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(recurrent_net, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Embedding(vocab_size, hidden_size)\n",
    "        \n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size)\n",
    "        \n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "       \n",
    "    def forward(self, word_seq, h, c ):\n",
    "        \n",
    "        temp_seq       =   self.encoder(word_seq)\n",
    "        \n",
    "        h_seq , (h,c)  =   self.rnn( temp_seq , (h,c) )\n",
    "        \n",
    "        score_seq      =   self.decoder( h_seq )\n",
    "        \n",
    "        return score_seq,  h , c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the net. Choose the hidden size to be 200. How many parameters in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recurrent_net(\n",
      "  (encoder): Embedding(33278, 200)\n",
      "  (rnn): LSTM(200, 200)\n",
      "  (decoder): Linear(in_features=200, out_features=33278, bias=True)\n",
      ")\n",
      "There are 13666078 (13.67 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "hidden_size=200\n",
    "\n",
    "net = recurrent_net( hidden_size )\n",
    "\n",
    "print(net)\n",
    "\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the weights of the networks to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up manually the weights of the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net.encoder.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "net.decoder.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the criterion, as well as the important hyperparameters: \n",
    "* batch size = 20\n",
    "* initial learning rate = 5\n",
    "* sequence length = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "bs  = 20\n",
    "\n",
    "my_lr = 5\n",
    "\n",
    "seq_length = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate the network on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "    c = torch.zeros(1, bs, hidden_size)\n",
    "   \n",
    "    h=h.to(device)\n",
    "    c=c.to(device)\n",
    "       \n",
    "    for count in range( 0 , test_length-1- seq_length ,  seq_length) :\n",
    "               \n",
    "        minibatch_data =  test_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
    "        \n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "                                  \n",
    "        scores, h, c  = net( minibatch_data, h , c)\n",
    "        \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length ) \n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)\n",
    "        \n",
    "        loss = criterion(  scores ,  minibatch_label )    \n",
    "        \n",
    "        h=h.detach()\n",
    "        c=c.detach()\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1        \n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    print('test perplexity = ', math.exp(total_loss)  )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do 8 passes through the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-29311bc82663>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thomas/Dropbox/06-DL-courses-Xavier-Thomas/Thomas/Thomas-DL/courses/02-LABS/20-RNN/utils.py\u001b[0m in \u001b[0;36mnormalize_gradient\u001b[0;34m(net)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mgrad_norm_sq\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mgrad_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_norm_sq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(8):\n",
    "    \n",
    "    # divide the learning rate by 3 except after the first epoch\n",
    "    if epoch >= 2:\n",
    "        my_lr = my_lr / 3\n",
    "    \n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quatities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    # set the initial h and c to be the zero vector\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "    c = torch.zeros(1, bs, hidden_size)\n",
    "\n",
    "    # send them to the gpu    \n",
    "    h=h.to(device)\n",
    "    c=c.to(device)\n",
    "    \n",
    "    for count in range( 0 , train_length-seq_length-1 ,  seq_length):\n",
    "             \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]        \n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "     \n",
    "        # tell Pytorch to start tracking all operations that will be done on h and c\n",
    "        h=h.requires_grad_()\n",
    "        c=c.requires_grad_()\n",
    "                       \n",
    "        # forward the minibatch through the net        \n",
    "        scores, h, c  = net( minibatch_data, h , c)\n",
    "        \n",
    "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        utils.normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # prevent from backpropagating all the way to the beginning\n",
    "        h=h.detach()\n",
    "        c=c.detach()\n",
    "            \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t loss=',  math.exp(total_loss))\n",
    "    eval_on_test_set() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text using the trained langage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the nation 's regulations <eos> the survey said it might grabbed the carrier 's primary pilot to intergroup the worry that parsow and incest were out of the past year <eos> western tennessee has proposing its <unk> british <unk> to william steinhardt and phillips owner related to construction customers <eos> the preamble 's borrowing will rise against many movies in the first soviet and europe <eos> and seven weeks later despite the gain of N N are expected to rise N N in N <eos> for the week began tuesday morning with a settlement of the hong kong capcom division to idle farmers in time students an ounce to a$ N a <eos> these revenue can not be replaced until the national <unk> market 's record and tenders of payments to market items that the series says remains brian <unk> by the end of the year <eos> in addition <unk> mr. dingell said it 's straight age and uncertain the turnaround purchasing for differences in between next and pacific leaves <eos> michael <unk> inc. said it would sell existing interests until three transaction of the dividend or dismissed of losses <eos> london insurance a group led by mr. lehman oil & co accountants whose earnings fell to $ N million or record a ton on the sale of a percentage of four health-care issues such as the globe and said it had no <unk> for fuel equity to paribas <eos> in west series itself has a corporation research separately withdrew a mural that miss fairly <unk> to several for nekoosa to buy jobs at a cray meeting <eos> commodore said it containing a aborted <unk> <unk> for the original <unk> act over the world <eos> but the company 's face larger assets is last week by <unk> hbo executives and others said mips will be taken back by letters in certain quarters <eos> declined to informed those agricultural government <unk> are under conduct <eos> several years ago by the end of the year it is unclear however attractive to the u.s. <unk> N who seemed differently on the <unk> <unk> and television equipment information <eos> and most of the earlier suggest that was still strike in the marketplace have had been wary <eos> this may have a cautious industry <eos> the standpoint imposed for interest rates not <eos> other airlines intended to get a very value voice in europe by mehta its electronic spots thomas <unk> 's president mark <unk> larry smith of smith john smith <eos> <unk> korea 's parent general foods <unk> offers all most of the more difficulties and <unk> <unk> between three loyal and run <unk> by a huge private company <eos> james gray a painewebber <unk> lead n.j. new producer thomas in three states had a director since N <eos> as far in talks about the rapid recruit block a debut last week 's bit inspection include a <unk> suspension of motel care was in the u.s. <eos> others also singled out <unk> saatchi "
     ]
    }
   ],
   "source": [
    "bs=1\n",
    "\n",
    "# initialize h and c to zero\n",
    "h = torch.zeros(1, bs, hidden_size)\n",
    "c = torch.zeros(1, bs, hidden_size)\n",
    "h=h.to(device)\n",
    "c=c.to(device)\n",
    "\n",
    "\n",
    "# pick the first word at random\n",
    "word_idx=torch.LongTensor(1,1).random_(0,vocab_size-1)\n",
    "word_idx=word_idx.to(device)\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    \n",
    "    # compute the scores used to predict what is the next word\n",
    "    score , h, c = net(word_idx , h, c)\n",
    "    \n",
    "    # sample\n",
    "    word_weights = score.squeeze().div(1).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    \n",
    "    # convert the word number to the actual string it corresponds to\n",
    "    word = corpus.dictionary.idx2word[word_idx]\n",
    "    print(word, end=' ')\n",
    "\n",
    "    # put back in the right format\n",
    "    word_idx=word_idx.view(1,1).to(device)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
