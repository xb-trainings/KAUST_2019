{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN -- DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import prepare_data\n",
    "import math\n",
    "import time\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder='./data/penn'\n",
    "\n",
    "corpus = prepare_data.Corpus(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    data = data.cuda()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46479, 20])\n",
      "torch.Size([4121, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/lib/python3.6/site-packages/torch/cuda/__init__.py:116: UserWarning: \n",
      "    Found GPU1 Quadro K600 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    }
   ],
   "source": [
    "batch_size=20\n",
    "bs=batch_size\n",
    "\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, batch_size)\n",
    "test_data = batchify(corpus.test, batch_size)\n",
    "\n",
    "vocab_size = len(corpus.dictionary)\n",
    "\n",
    "print(train_data.size())\n",
    "print(test_data.size())\n",
    "\n",
    "train_length = train_data.size(0)\n",
    "test_length  = test_data.size(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With or without GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "#device= torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a recurrent net class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class thre_layer_recurrent_net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(three_layer_recurrent_net, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Embedding( vocab_size   , hidden_size  )       \n",
    "        self.layer2 = nn.RNN(       hidden_size  , hidden_size  ) \n",
    "        self.layer3 = nn.Linear(    hidden_size  , vocab_size   )\n",
    "\n",
    "       \n",
    "    def forward(self, word_seq, h_init ):\n",
    "        \n",
    "        g_seq            =   self.layer1(word_seq)\n",
    "        h_seq , h_final  =   self.layer2( g_seq , h_init )\n",
    "        score_seq        =   self.layer3( h_seq )\n",
    "        \n",
    "        return score_seq,  h_final \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the net. Choose the hidden size to be 200. How many parameters in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recurrent_net(\n",
      "  (encoder): Embedding(10000, 200)\n",
      "  (rnn): LSTM(200, 200)\n",
      "  (decoder): Linear(in_features=200, out_features=10000, bias=True)\n",
      ")\n",
      "There are 4331600 (4.33 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "hidden_size=200\n",
    "\n",
    "net = recurrent_net( hidden_size )\n",
    "\n",
    "print(net)\n",
    "\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the weights of the networks to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up manually the weights of the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net.encoder.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "net.decoder.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the criterion, as well as the important hyperparameters: \n",
    "* batch size = 20\n",
    "* initial learning rate = 5\n",
    "* sequence length = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "bs  = 20\n",
    "\n",
    "my_lr = 5\n",
    "\n",
    "seq_length = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate the network on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "    c = torch.zeros(1, bs, hidden_size)\n",
    "   \n",
    "    h=h.to(device)\n",
    "    c=c.to(device)\n",
    "       \n",
    "    for count in range( 0 , test_length-1- seq_length ,  seq_length) :\n",
    "               \n",
    "        minibatch_data =  test_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
    "        \n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "                                  \n",
    "        scores, h, c  = net( minibatch_data, h , c)\n",
    "        \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length ) \n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)\n",
    "        \n",
    "        loss = criterion(  scores ,  minibatch_label )    \n",
    "        \n",
    "        h=h.detach()\n",
    "        c=c.detach()\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1        \n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    print('test perplexity = ', math.exp(total_loss)  )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do 8 passes through the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch= 0 \t time= 24.20134663581848 \t lr= 5 \t loss= 285.92059756946037\n",
      "test perplexity =  183.81680528935263\n",
      "\n",
      "epoch= 1 \t time= 48.9482638835907 \t lr= 5 \t loss= 135.49374076235077\n",
      "test perplexity =  141.53843569456083\n",
      "\n",
      "epoch= 2 \t time= 73.8051323890686 \t lr= 1.6666666666666667 \t loss= 89.8852607294775\n",
      "test perplexity =  118.5896965718748\n",
      "\n",
      "epoch= 3 \t time= 98.58819055557251 \t lr= 0.5555555555555556 \t loss= 75.81924739254758\n",
      "test perplexity =  113.7814865221798\n",
      "\n",
      "epoch= 4 \t time= 123.43047738075256 \t lr= 0.1851851851851852 \t loss= 71.02879274726945\n",
      "test perplexity =  112.06782514215153\n",
      "\n",
      "epoch= 5 \t time= 148.29536271095276 \t lr= 0.0617283950617284 \t loss= 69.30741699769199\n",
      "test perplexity =  111.32622582768332\n",
      "\n",
      "epoch= 6 \t time= 173.05349683761597 \t lr= 0.0205761316872428 \t loss= 68.68876516757364\n",
      "test perplexity =  110.93527339260221\n",
      "\n",
      "epoch= 7 \t time= 197.9068443775177 \t lr= 0.006858710562414266 \t loss= 68.46669010908256\n",
      "test perplexity =  110.72453037047049\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(8):\n",
    "    \n",
    "    # divide the learning rate by 3 except after the first epoch\n",
    "    if epoch >= 2:\n",
    "        my_lr = my_lr / 3\n",
    "    \n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quatities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    # set the initial h and c to be the zero vector\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "    c = torch.zeros(1, bs, hidden_size)\n",
    "\n",
    "    # send them to the gpu    \n",
    "    h=h.to(device)\n",
    "    c=c.to(device)\n",
    "    \n",
    "    for count in range( 0 , train_length-seq_length-1 ,  seq_length):\n",
    "             \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]        \n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "     \n",
    "        # tell Pytorch to start tracking all operations that will be done on h and c\n",
    "        h=h.requires_grad_()\n",
    "        c=c.requires_grad_()\n",
    "                       \n",
    "        # forward the minibatch through the net        \n",
    "        scores, h, c  = net( minibatch_data, h , c)\n",
    "        \n",
    "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        utils.normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # prevent from backpropagating all the way to the beginning\n",
    "        h=h.detach()\n",
    "        c=c.detach()\n",
    "            \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t loss=',  math.exp(total_loss))\n",
    "    eval_on_test_set() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text using the trained langage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the nation 's regulations <eos> the survey said it might grabbed the carrier 's primary pilot to intergroup the worry that parsow and incest were out of the past year <eos> western tennessee has proposing its <unk> british <unk> to william steinhardt and phillips owner related to construction customers <eos> the preamble 's borrowing will rise against many movies in the first soviet and europe <eos> and seven weeks later despite the gain of N N are expected to rise N N in N <eos> for the week began tuesday morning with a settlement of the hong kong capcom division to idle farmers in time students an ounce to a$ N a <eos> these revenue can not be replaced until the national <unk> market 's record and tenders of payments to market items that the series says remains brian <unk> by the end of the year <eos> in addition <unk> mr. dingell said it 's straight age and uncertain the turnaround purchasing for differences in between next and pacific leaves <eos> michael <unk> inc. said it would sell existing interests until three transaction of the dividend or dismissed of losses <eos> london insurance a group led by mr. lehman oil & co accountants whose earnings fell to $ N million or record a ton on the sale of a percentage of four health-care issues such as the globe and said it had no <unk> for fuel equity to paribas <eos> in west series itself has a corporation research separately withdrew a mural that miss fairly <unk> to several for nekoosa to buy jobs at a cray meeting <eos> commodore said it containing a aborted <unk> <unk> for the original <unk> act over the world <eos> but the company 's face larger assets is last week by <unk> hbo executives and others said mips will be taken back by letters in certain quarters <eos> declined to informed those agricultural government <unk> are under conduct <eos> several years ago by the end of the year it is unclear however attractive to the u.s. <unk> N who seemed differently on the <unk> <unk> and television equipment information <eos> and most of the earlier suggest that was still strike in the marketplace have had been wary <eos> this may have a cautious industry <eos> the standpoint imposed for interest rates not <eos> other airlines intended to get a very value voice in europe by mehta its electronic spots thomas <unk> 's president mark <unk> larry smith of smith john smith <eos> <unk> korea 's parent general foods <unk> offers all most of the more difficulties and <unk> <unk> between three loyal and run <unk> by a huge private company <eos> james gray a painewebber <unk> lead n.j. new producer thomas in three states had a director since N <eos> as far in talks about the rapid recruit block a debut last week 's bit inspection include a <unk> suspension of motel care was in the u.s. <eos> others also singled out <unk> saatchi "
     ]
    }
   ],
   "source": [
    "bs=1\n",
    "\n",
    "# initialize h and c to zero\n",
    "h = torch.zeros(1, bs, hidden_size)\n",
    "c = torch.zeros(1, bs, hidden_size)\n",
    "h=h.to(device)\n",
    "c=c.to(device)\n",
    "\n",
    "\n",
    "# pick the first word at random\n",
    "word_idx=torch.LongTensor(1,1).random_(0,vocab_size-1)\n",
    "word_idx=word_idx.to(device)\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    \n",
    "    # compute the scores used to predict what is the next word\n",
    "    score , h, c = net(word_idx , h, c)\n",
    "    \n",
    "    # sample\n",
    "    word_weights = score.squeeze().div(1).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    \n",
    "    # convert the word number to the actual string it corresponds to\n",
    "    word = corpus.dictionary.idx2word[word_idx]\n",
    "    print(word, end=' ')\n",
    "\n",
    "    # put back in the right format\n",
    "    word_idx=word_idx.view(1,1).to(device)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
