{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM -- SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With or without GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "#device= torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Penn Tree Bank (the tensor train_data should consists of 20 columns of ~50,000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46479, 20])\n",
      "torch.Size([4121, 20])\n"
     ]
    }
   ],
   "source": [
    "train_data  =  torch.load('../../data/PTB/data/train_data.pt')\n",
    "test_data   =  torch.load('../../data/PTB/data/test_data.pt')\n",
    "\n",
    "print(  train_data.size()  )\n",
    "print(  test_data.size()   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some constants associated with the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs = 20\n",
    "\n",
    "vocab_size = 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a recurrent net class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class three_layer_recurrent_net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(three_layer_recurrent_net, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n",
    "        self.layer2 = nn.LSTM(      hidden_size , hidden_size  )\n",
    "        self.layer3 = nn.Linear(    hidden_size , vocab_size   )\n",
    "\n",
    "        \n",
    "    def forward(self, word_seq, h_init, c_init ):\n",
    "        \n",
    "        g_seq                      =   self.layer1( word_seq )  \n",
    "        h_seq , (h_final,c_final)  =   self.layer2( g_seq , (h_init,c_init) )      \n",
    "        score_seq                  =   self.layer3( h_seq )\n",
    "        \n",
    "        return score_seq,  h_final , c_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the net. Choose the hidden size to be 300. How many parameters in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three_layer_recurrent_net(\n",
      "  (layer1): Embedding(10000, 300)\n",
      "  (layer2): LSTM(300, 300)\n",
      "  (layer3): Linear(in_features=300, out_features=10000, bias=True)\n",
      ")\n",
      "There are 6732400 (6.73 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "hidden_size=300\n",
    "\n",
    "net = three_layer_recurrent_net( hidden_size )\n",
    "\n",
    "print(net)\n",
    "\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the weights of the networks to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up manually the weights of the embedding module and Linear module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net.layer1.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "net.layer3.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the criterion, as well as the following important hyperparameters: \n",
    "* initial learning rate = 5\n",
    "* sequence length = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "my_lr = 5\n",
    "\n",
    "seq_length = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate the network on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "    c = torch.zeros(1, bs, hidden_size)\n",
    "   \n",
    "    h=h.to(device)\n",
    "    c=c.to(device)\n",
    "       \n",
    "    for count in range( 0 , 4120-seq_length ,  seq_length) :\n",
    "               \n",
    "        minibatch_data =  test_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
    "        \n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "                                  \n",
    "        scores, h, c  = net( minibatch_data, h , c)\n",
    "        \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length ) \n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)\n",
    "        \n",
    "        loss = criterion(  scores ,  minibatch_label )    \n",
    "        \n",
    "        h=h.detach()\n",
    "        c=c.detach()\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1        \n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do 8 passes through the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch= 0 \t time= 13.378747701644897 \t lr= 5 \t exp(loss)= 278.80329535959396\n",
      "test: exp(loss) =  176.10091439623466\n",
      "\n",
      "epoch= 1 \t time= 27.119251489639282 \t lr= 5 \t exp(loss)= 127.2893813062593\n",
      "test: exp(loss) =  133.56880609067883\n",
      "\n",
      "epoch= 2 \t time= 41.00815463066101 \t lr= 1.6666666666666667 \t exp(loss)= 81.5507055656398\n",
      "test: exp(loss) =  114.37825619482055\n",
      "\n",
      "epoch= 3 \t time= 54.876781940460205 \t lr= 0.5555555555555556 \t exp(loss)= 67.34177944883491\n",
      "test: exp(loss) =  110.70630542720009\n",
      "\n",
      "epoch= 4 \t time= 69.02088618278503 \t lr= 0.1851851851851852 \t exp(loss)= 62.48224239962807\n",
      "test: exp(loss) =  109.20213748226952\n",
      "\n",
      "epoch= 5 \t time= 83.68946361541748 \t lr= 0.0617283950617284 \t exp(loss)= 60.721349522223306\n",
      "test: exp(loss) =  108.41556901626859\n",
      "\n",
      "epoch= 6 \t time= 98.10883212089539 \t lr= 0.0205761316872428 \t exp(loss)= 60.07901541829639\n",
      "test: exp(loss) =  107.99166749728593\n",
      "\n",
      "epoch= 7 \t time= 112.47274374961853 \t lr= 0.006858710562414266 \t exp(loss)= 59.84645040000237\n",
      "test: exp(loss) =  107.73571779154165\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(8):\n",
    "    \n",
    "    # divide the learning rate by 3 except after the first epoch\n",
    "    if epoch >= 2:\n",
    "        my_lr = my_lr / 3\n",
    "    \n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quatities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    # set the initial h and c to be the zero vector\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "    c = torch.zeros(1, bs, hidden_size)\n",
    "\n",
    "    # send them to the gpu    \n",
    "    h=h.to(device)\n",
    "    c=c.to(device)\n",
    "    \n",
    "    for count in range( 0 , 46478-seq_length ,  seq_length):\n",
    "        \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]        \n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        # Detach to prevent from backpropagating all the way to the beginning\n",
    "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
    "        h=h.detach()\n",
    "        c=c.detach()\n",
    "        h=h.requires_grad_()\n",
    "        c=c.requires_grad_()\n",
    "                       \n",
    "        # forward the minibatch through the net        \n",
    "        scores, h, c  = net( minibatch_data, h , c)\n",
    "        \n",
    "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        utils.normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "            \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "    eval_on_test_set() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text using the trained langage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "krenz asserts they began <unk> by exceeds two firms in parliament <eos> walter <unk> an official at texas air corp. and nbc intelligence union finance in metromedia said that manufactured competition anti-abortion <unk> <unk> increases rate on medical expenses and unemployment <eos> that happy authority runs <unk> members than the expense should be plays snapped into the south sea <eos> diet partner <eos> white house plans for most leveraged ventures all moscow wo n't boost <unk> laws if a local party will <unk> their portions of funds to cover those extent for their bills <eos> as a result the japanese government is now paying up an injunction in hutchinson stations in california that some figures are trying to cope with other currencies but said that price caused by the radio guard and very heavy use of <unk> would garden noriega shortages if any democrats cut upper <unk> admits <eos> in national over-the-counter trading yesterday new york the $ <unk> central agreement on the internal revenue service said the company had expected that it expects to report earnings because of its debt and lay off a high-risk term contract for the 10-year division <eos> an explosion was n't and unlike a analyzing acquisition strategy while recommended stock prices increased combined pakistan 's accomplished from earnings <eos> we estimated that the number of dollars currently neighborhood and are n't invariably the purchasing industry 's ferranti known as pricing <eos> maryland is expecting strong plans to acquire the first asset value of N <eos> moreover consumers add litigation with payment more money this accounting for the bill are advocates of sense <eos> it 's a breakdown of art says one management economist <eos> the reupke order will be avoided by the end of the year and said neither walter <unk> president of the parent company on the over-the-counter industry as often <unk> houses <eos> treasury bonds with the midwest financial unit is proposing to parallel individual investors <eos> now there 's a story a company is being won again <eos> the italian operator cited importing things to <unk> its <unk> business <eos> we 're across the gainers last summer <eos> the <unk> psychological rate free termination mr. lang said <eos> a spokesman for talk of the trade <eos> the bad amount eventually could be easy next month because the fed does n't violate the backdrop of debt <eos> among industry analysts and directors would confirm the outcome of the project which are spending law <eos> but the ftc is currently directors to build up pitches in favor of options for market conditions in permanent enabling vehicle bank regulatory financing for spun data agreements to <unk> from nbi 's <unk> market <eos> critics note that affects subscribe rates will also continue to face in the program for many years <eos> further intel is unusual and financial telesis development bonds are larger <eos> such research is way to further pay for zeta and build a drug research at its ability to take the agriculture talks "
     ]
    }
   ],
   "source": [
    "idx2word  =  torch.load('../../data/PTB/data/idx2word.pt')\n",
    "\n",
    "bs=1\n",
    "\n",
    "# initialize h and c to zero\n",
    "h = torch.zeros(1, bs, hidden_size)\n",
    "c = torch.zeros(1, bs, hidden_size)\n",
    "h=h.to(device)\n",
    "c=c.to(device)\n",
    "\n",
    "# pick the first word at random\n",
    "word_idx=torch.LongTensor(1,1).random_(0,vocab_size-1)\n",
    "word_idx=word_idx.to(device)\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    \n",
    "    # compute the scores used to predict what is the next word\n",
    "    scores , h, c = net(word_idx , h, c)\n",
    "    \n",
    "    # use a softmax to get a probability distribution over the vocabulary \n",
    "    probs=F.softmax(scores,dim=2)\n",
    "    \n",
    "    # sample\n",
    "    probs=probs.squeeze().cpu()\n",
    "    word_idx = torch.multinomial(probs, 1)\n",
    "    \n",
    "    # sample with temperature\n",
    "    #  word_weights = scores.squeeze().div(1).exp().cpu()\n",
    "    #  word_idx = torch.multinomial(word_weights, 1)\n",
    "    \n",
    "    # convert the word number to the actual string it corresponds to\n",
    "    word = idx2word[ word_idx.item() ]\n",
    "    print(word, end=' ')\n",
    "\n",
    "    # put back in the right format\n",
    "    word_idx=word_idx.view(1,1).to(device)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
